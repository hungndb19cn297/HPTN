{"content":"<h1 id=\"1giithiu\">1. GIỚI THIỆU</h1><p>Trong những năm gần đây, với sự phát triển của các mô hình <strong>Generative model</strong> các ứng dụng tự sinh dữ liệu text, video, ảnh càng trở nên nhiều hơn. Các ứng dụng chỉnh sửa ảnh và làm đẹp cũng được chú trọng trên các ứng dụng app, web như:  <a href=\"https://play.google.com/store/apps/details?id=com.instashot.camerashot.mediaidea\">TakeShot AI Video Photo Editor</a> , faceapp, gradient, Perfect365 Makeup, Beauty Makeup, facetune2 mục tiêu để có những bức ảnh đep chia sẻ trên mạng xã hội. Trong bài viết này, mình sẽ chia sẻ về cách để tự sinh ra những bức ảnh avatar từ AI sử dụng stable diffusion.<br/>Let go!!!!</p><h1 id=\"2tvn\">2. ĐẶT VẤN ĐỀ</h1><p>Trước khi triển khai, chúng ta sẽ tìm hiểu qua về mô hình Stable Diffusion và tiền thân của nó là mô hình Diffusion nhé.</p><h2 id=\"21mhnhdiffusion\">2.1. Mô hình Diffusion</h2><p>Diffusion Models (DMs) là một mô hình xác suất, hoạt động bằng cách ứng dụng Unet khử nhiễu lặp đi lặp lại nhiều lần để tạo ra ảnh thực từ nhiễu. <br/>Quá trình huấn luyện của mô hình diffusion bao gồm hai giai đoạn:</p><ul><li>Forward Diffusion Process: Mô hình sẽ được huấn luyện từ ảnh ban đầu với kích thước <strong>512*512pixel</strong> cùng với quá trình khuấn tán thuận, mô hình sẽ thêm nhiễu (noise) theo phân phối $q(x<em>{1:T} | x</em>0)$ cho đến khi đầu ra là một ảnh toàn nhiễu.</li><li>Reverse Diffusion Process: khử nhiễu từ ảnh từng bước một theo phân phối $p<em>{\\theta}(x</em>{0:T})$, đây chính là giai đoạn mà mô hình diffusion sẽ tìm cách học để đảo ngược quá trình thêm nhiễu vào trong ảnh, từ đó có thể tạo ra ảnh thực từ nhiễu.</li></ul><p><img src=\"https://images.viblo.asia/cee14713-e03b-427d-a0d3-a278e5947430.png\" alt=\"image.png\" /> </p><p>&lt;div align=\"center\"&gt;Hình 1: Hình ảnh mô hình diffusion<br/>&lt;/div&gt;</p><p>Mô hình diffusion cho kết quả tốt so các generative models như GAN, VAE, tuy nhiên mô tả cho đến nay sẽ tạo ra hình ảnh nhưng không sử dụng bất kỳ  điều kiện ràng buộc nào. Vì vậy, nếu chúng ta triển khai mô hình này, nó sẽ tạo ra những hình ảnh đẹp mắt, nhưng không có cách nào kiểm soát. Để hiểu rõ hơn về mô hình bạn có thể tham khảo: <a href=\"https://arxiv.org/abs/2006.11239\">Denoising Diffusion Probabilistic Models</a> .</p><h2 id=\"22mhnhstablediffusion\">2.2 Mô hình Stable Diffusion</h2><p>Stable Diffusion (Latent Diffusion Model) được giới thiệu trong bài báo <a href=\"https://arxiv.org/abs/2112.10752\">High-Resolution Image Synthesis with Latent Diffusion Models (Rombach et al., 2022)</a> và đạt kết quả tương đối tốt trên nhiều tác vụ khác nhau như unconditional image generation (sinh ảnh không điều kiện), semantic scene synthesis (sinh ảnh từ segmentation mask), and super-resolution (tăng độ phân giải cho ảnh), trong khi chạy nhanh hơn và cần ít tài nguyên tính toán hơn so với mô hình diffusion gốc. Để làm được điều đó thì Stable Diffusion sử dụng phần encoder của một autoencoder để nén ảnh dưới dạng lower-dimensional representations (biểu diễn ít chiều hơn) trong không gian pixel mô tả như hình và  latent space (không gian dữ liệu ẩn), rồi cho qua quá trình tương tự như ảnh trong mô hình diffusion gốc, sau đó sử dụng phần decoder của autoencoder để giải nén latent data trở về ảnh.</p><p><img src=\"https://images.viblo.asia/ed474bac-123a-42b2-9097-a3196f4494d3.png\" alt=\"Screenshot from 2023-08-10 17-44-41.png\" /></p><p>&lt;div align=\"center\"&gt;Hình 2. Mô hình Stable Diffusion&lt;/div&gt;</p><p>Ta có thế nhận thấy ở mục 2.1 mô hình diffusion Unet không sử dụng điều kiện(conditioning) đầu vào và đầu ra sẽ như hình:</p><p><img src=\"https://images.viblo.asia/1103ca2c-7df0-4858-a2be-8d0d79e512eb.png\" alt=\"image.png\" /></p><p>&lt;div align=\"center\"&gt;Hình 3. Các lớp của bộ dự đoán nhiễu Unet (không có điều kiện)&lt;/div&gt;<br/>Bên trong, chúng ta thấy rằng:</p><ul><li>Unet là một loạt các lớp để biến đổi mảng latent</li><li>Mỗi lớp thực hiện biến đổi trên đầu ra của lớp trước</li><li>Một số đầu ra được chuyển tiếp (thông qua các kết nối residual) vào quá trình xử lý sau đó trong mạng.</li><li>Timestep được chuyển đổi thành một timestep embedding vector và được sử dụng trong các lớp.</li></ul><p>Còn với Stable Diffusion còn cho phép sinh ảnh từ text prompts (đoạn văn mô tả) bằng cách chuyển chúng thành text embeddings thông qua việc sử dụng mô hình ngôn ngữ (ví dụ như BERT, CLIP) rồi đưa chúng vào trong Unet thông qua multihead attention layer. Mô hình cho phép sáng tạo nội dung dựa trên prompt từ người dùng.</p><p><img src=\"https://images.viblo.asia/6a6793e4-7cd5-46f6-9ec6-41d049554fff.png\" alt=\"image.png\" /></p><p>&lt;div align=\"center\"&gt;Hình 4. Các lớp của bộ dự đoán nhiễu Unet (có điều kiện)&lt;/div&gt;</p><h1 id=\"3trinkhaibiton\">3 TRIỂN KHAI BÀI TOÁN</h1><h2 id=\"31chunbdliu\">3.1. Chuẩn bị dữ liệu</h2><p>Để có kết quả tốt cho việc training thì việc chuẩn bị dữ liệu ảnh đầu vào là quan trọng. Để có thể sinh ảnh giống với chủ thể trong khung hình bạn cần chuẩn bị một thư viện ảnh gồm 10 -20 hình ảnh thật rõ nét và chụp các góc cạnh khác nhau của đối tượng, chủ thể và trong những background khác nhau. Hãy đảm bảo rằng tập hình ảnh của bạn sẽ có kích thước  <strong>512x512pixel</strong> , để chuyển hình ảnh bạn có thể sử dụng web resize ảnh online <a href=\"https://www.birme.net/?no_resize=true&amp;auto_focal=false\">BIRME</a> như hình 5</p><p><img src=\"https://images.viblo.asia/92b0391a-a6f0-4490-b2fb-87852f00ab50.png\" alt=\"image.png\" /></p><p>&lt;div align=\"center\"&gt;Hình 5. Dữ liệu hình ảnh crop về kích thước 512x512pixel&lt;/div&gt;</p><h2 id=\"32training\">3.2. Training</h2><p>Mình sử dụng Google Colab trong quá trình training mô hình DreamBooth tự sinh với chính tập dữ liệu của mình và để không bị ngắt kết nối bạn nên dùng tài khoản gmail mới để thực hiện, hoặc bạn có thể nâng cấp lên gói Google Colab Pro nếu cần đào tạo dữ liệu lớn hơn. Link Google Colab mình để ở dưới: <a href=\"https://colab.research.google.com/github/FurkanGozukara/Stable-Diffusion/blob/main/DreamBooth/ShivamShriraoDreamBooth.ipynb#scrollTo=5vDpCxId1aCm\">source code</a>Chúng ta cùng đi vào thực hiện thôi nào 😁😁😁</p><h3 id=\"bc1ktningoogledrive\">Bước 1: Kết nối đến Google Drive</h3><pre><code>from google.colab import drive<br/>drive.mount('/content/drive')<br/></code></pre><h3 id=\"bc2citccthvincnthit\">Bước 2: Cài đặt các thư viện cần thiết</h3><pre><code>!wget -q https://gist.githubusercontent.com/FurkanGozukara/be7be5f9f7820d0bb85a3052874f184e/raw/d8d179da6cab0735bd5832029c2dec5163db87b4/train_dreambooth.py<br/>!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py<br/>%pip install -qq git+https://github.com/ShivamShrirao/diffusers<br/>%pip install -q -U --pre triton<br/>%pip install -q accelerate transformers ftfy bitsandbytes==0.35.0 gradio natsort safetensors xformers<br/>%pip uninstall torch -y<br/>%pip uninstall torchvision -y<br/>%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118<br/></code></pre><h3 id=\"bc3downloadstablediffusionweights\">Bước 3:  Download Stable Diffusion weights</h3><p>Bạn cần đăng ký tài khoản  <strong>HuggingFace</strong> 🤗 để có thể tải về các file Stable Diffusion weights.</p><pre><code>MODEL_NAME = \"runwayml/stable-diffusion-v1-5\" #@param {type:\"string\"}<br/>#@markdown Enter the directory name to save model at.<br/>OUTPUT_DIR = \"stable_diffusion_weights/ohwx\" #@param {type:\"string\"}<br/>if save_to_gdrive:<br/>    OUTPUT_DIR = \"/content/drive/MyDrive/\" + OUTPUT_DIR<br/>else:<br/>    OUTPUT_DIR = \"/content/\" + OUTPUT_DIR<br/>print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")<br/>!mkdir -p $OUTPUT_DIR<br/></code></pre><p>Ở bài viết này mình sử dụng model <strong>stable-diffusion-v1-5</strong>, sau khi tải về sẽ được lưu vào google drive với đường dẫn là <em>OUTPUTDIR</em></p><h3 id=\"bc4cuhnhfileconfig\">Bước 4:  Cấu hình file config</h3><p>Phần này khá quan trọng vì nó định nghĩa đối tượng chủ thể như ví dụ: đối tượng là <em>hoanganh</em> và class_prompt là <em>person</em>.</p><pre><code>concepts_list = [<br/>    {<br/>        \"instance_prompt\":      \"hoanganh\",<br/>        \"class_prompt\":         \"person\",<br/>        \"instance_data_dir\":    \"/content/data/hoanganh\",<br/>        \"class_data_dir\":       \"/content/data/persion\"<br/>    },<br/>]<br/># `class_data_dir` contains regularization images<br/>import json<br/>import os<br/>for c in concepts_list:<br/>    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)<br/>with open(\"concepts_list.json\", \"w\") as f:<br/>    json.dump(concepts_list, f, indent=4)<br/></code></pre><h3 id=\"bc5training\">Bước 5:  Training</h3><p>Quá trình training model sử dụng Dreambooth trong Stable Diffusion là một trong những cách để giúp bạn có một bộ mô hình riêng của cá nhân mình. Thông thường bạn sẽ phải tải các model checkpoint hay lora được chia sẻ trên cộng đồng mạng để vẽ tranh AI trong SD. Nhưng với việc train Dreambooth bạn sẽ thỏa sức sáng tạo, và tạo ra những hình ảnh AI theo phong cách của bạn.</p><pre><code>!python3 train_dreambooth.py \\<br/>  --pretrained_model_name_or_path=$MODEL_NAME \\<br/>  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\<br/>  --output_dir=$OUTPUT_DIR \\<br/>  --revision=\"fp16\" \\<br/>  --with_prior_preservation --prior_loss_weight=1.0 \\<br/>  --seed=1337 \\<br/>  --resolution=512 \\<br/>  --train_batch_size=1 \\<br/>  --train_text_encoder \\<br/>  --mixed_precision=\"fp16\" \\<br/>  --use_8bit_adam \\<br/>  --gradient_accumulation_steps=1 \\<br/>  --learning_rate=1e-6 \\<br/>  --lr_scheduler=\"constant\" \\<br/>  --lr_warmup_steps=0 \\<br/>  --num_class_images=50 \\<br/>  --sample_batch_size=1 \\<br/>  --max_train_steps=800 \\<br/>  --save_interval=10000 \\<br/>  --save_sample_prompt=\"hoanganh person\" \\<br/>  --concepts_list=\"concepts_list.json\"<br/></code></pre><h3 id=\"bc6previewktqusaukhioto\">Bước 6:  Preview kết quả sau khi đào tạo</h3><pre><code>import os<br/>import matplotlib.pyplot as plt<br/>import matplotlib.image as mpimg<br/>weights_folder = OUTPUT_DIR<br/>folders = sorted([f for f in os.listdir(weights_folder) if f != \"0\"], key=lambda x: int(x))<br/>row = len(folders)<br/>col = len(os.listdir(os.path.join(weights_folder, folders[0], \"samples\")))<br/>scale = 4<br/>fig, axes = plt.subplots(row, col, figsize=(col*scale, row*scale), gridspec_kw={'hspace': 0, 'wspace': 0})<br/>for i, folder in enumerate(folders):<br/>    folder_path = os.path.join(weights_folder, folder)<br/>    image_folder = os.path.join(folder_path, \"samples\")<br/>    images = [f for f in os.listdir(image_folder)]<br/>    for j, image in enumerate(images):<br/>        if row == 1:<br/>            currAxes = axes[j]<br/>        else:<br/>            currAxes = axes[i, j]<br/>        if i == 0:<br/>            currAxes.set_title(f\"Image {j}\")<br/>        if j == 0:<br/>            currAxes.text(-0.1, 0.5, folder, rotation=0, va='center', ha='center', transform=currAxes.transAxes)<br/>        image_path = os.path.join(image_folder, image)<br/>        img = mpimg.imread(image_path)<br/>        currAxes.imshow(img, cmap='gray')<br/>        currAxes.axis('off')<br/>plt.tight_layout()<br/>plt.savefig('grid.png', dpi=72)<br/></code></pre><p>Kết quả là hình ảnh của chủ thể được sinh ra như hình 6</p><p><img src=\"https://images.viblo.asia/7b957c6b-4f12-4e82-a0d8-625393994f76.png\" alt=\"Screenshot from 2023-08-11 13-32-49.png\" /></p><p>&lt;div align=\"center\"&gt;Hình 6. Hình chủ thể được sinh ra&lt;/div&gt;</p><h3 id=\"bc7inference\">Bước 7:  Inference</h3><p>Sử dụng model sau khi đã training DreamBooth xong làm đầu vào cho mô hình Stable Diffusion nào. Bạn hãy chuẩn bị một số prompt để có thể kiểm tra các kết quả trả ra nhé. Có thể tham khảo các prompt free tại đây: <a href=\"https://lexica.art/\">lexica</a>.</p><pre><code>import torch<br/>from torch import autocast<br/>from diffusers import StableDiffusionPipeline, DDIMScheduler<br/>from IPython.display import display<br/>model_path = '/content/stable_diffusion_weights/ohwx/800'             # If you want to use previously trained model saved in gdrive, replace this with the full path of model in gdrive<br/>pipe = StableDiffusionPipeline.from_pretrained(model_path, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")<br/>pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)<br/>pipe.enable_xformers_memory_efficient_attention()<br/>g_cuda = None<br/>prompt = \"photo of ohwx man in tomer hanuka style\" #@param {type:\"string\"}<br/>negative_prompt = \"\" #@param {type:\"string\"}<br/>num_samples = 4 #@param {type:\"number\"}<br/>guidance_scale = 7.5 #@param {type:\"number\"}<br/>num_inference_steps = 24 #@param {type:\"number\"}<br/>height = 512 #@param {type:\"number\"}<br/>width = 512 #@param {type:\"number\"}<br/>with autocast(\"cuda\"), torch.inference_mode():<br/>    images = pipe(<br/>        prompt,<br/>        height=height,<br/>        width=width,<br/>        negative_prompt=negative_prompt,<br/>        num_images_per_prompt=num_samples,<br/>        num_inference_steps=num_inference_steps,<br/>        guidance_scale=guidance_scale,<br/>        generator=g_cuda<br/>    ).images<br/>for img in images:<br/>    display(img)<br/></code></pre><p>Trong đó:</p><ul><li>Prompt: là input đầu vào chính được sử dụng để hướng dẫn việc tạo ra hình ảnh mong muốn</li><li>height, width: kích thước hình ảnh sinh ra tương ứng.</li><li>negative_prompt: có thể được sử dụng để chỉ ra các khía cạnh mà bạn muốn tránh hoặc giảm thiểu trong các hình ảnh được tạo ra. VD: \"buồn bã\", \"nền tối\", …</li><li>num<em>images</em>per_prompt: số lượng ảnh mong muốn được sinh ra.</li><li>guidance_scale: là một tham số kiểm soát mức độ quá trình tạo hình ảnh tuân theo prompt. Giá trị càng cao, hình ảnh càng dính vào một đầu vào văn bản nhất định</li></ul><p>Đây là kết quả AI Avatar của mình sinh ra từ Stable Diffusion với  8 mẫu prompt.</p><p><img src=\"https://images.viblo.asia/f63db10d-b27d-4013-8247-dac1527594e4.png\" alt=\"Screenshot from 2023-08-11 16-14-30.png\" /> </p><p>&lt;div align=\"center\"&gt;Hình 7. Kết quả trả tra sau khi qua Stable Diffusion&lt;/div&gt;</p><h1 id=\"4ktlun\">4. KẾT LUẬN</h1><p>Trên đây là hướng dẫn về cách sử dụng Stable Diffusion sinh ra các hình ảnh Avatar theo mong muốn của mọi người rồi nhé. <br/>Hãy tận dụng hết sức mạnh của công nghệ trí tuệ nhân tạo A.I để sáng tạo ra bất cứ thứ gì mà bạn thích. Với Dreambooth và Stable Diffusion hứa hẹn sẽ đóng góp vào phát triển của cộng đồng Machine Learning và Deep Learning, giúp cải thiện hiệu suất của các mô hình học máy và ứng dụng của chúng trong thực tế.</p><h1 id=\"5tiliuthamchiu\">5. TÀI LIỆU THAM CHIẾU</h1><p>[1] <a href=\"https://trituenhantao.io/kien-thuc/minh-hoa-stable-diffusion/\">Minh Họa Stable Diffusion</a></p><p>[2] <a href=\"https://arxiv.org/abs/2006.11239\">Denoising Diffusion Probabilistic Models</a></p><p>[3] <a href=\"https://aichatgpt.vn/huong-dan-training-model-voi-dreambooth-trong-stable-diffusion/\">Dreambooth-trong-stable-diffusion</a></p><p>[4] <a href=\"https://theaisummer.com/diffusion-models/\">Diffusion models</a></p><p>[5] https://viblo.asia/p/paper-explain-high-resolution-image-synthesis-with-latent-diffusion-models-r1QLxPogLAw </p><p>[6] <a href=\"https://arxiv.org/abs/2112.10752\">High-Resolution Image Synthesis with Latent Diffusion Models</a></p><p>[7] <a href=\"https://arxiv.org/abs/2208.11970\">Understanding Diffusion Models: A Unified Perspective</a></p>","title":"[AI-Avatar] Tạo sinh Avatar cùng với Stable Diffusion","tags":["Deep Learning","Stable Diffusion","Generative Adversarial Network","Computer Vision","diffusion-model"],"created_at":1691746685000,"updated_at":1691983269000,"comments":[]}
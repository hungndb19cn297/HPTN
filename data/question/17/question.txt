{"content":"<h1 id=\"1nglc\">1. Động lực</h1><p>Các hệ thống gợi ý tin tức giúp cho người dùng tìm được những tin bài mà họ thật sự muốn quan tâm. Việc mô hình hóa chính xác các tin tức và thông tin user là rất cần thiết cho hệ thống gợi ý tin tức. Đặc biệt, nắm bắt được context của các từ và tin bài là ý tưởng chủ chốt để học được những biểu diễn của tin bài và user.</p><h1 id=\"2nggp\">2. Đóng góp</h1><p>Nhóm tác giả đề xuất cách tiếp cận mô hình hóa tin bài sử dụng multi-head self-attention (NRMS). Cốt lõi của cách tiếp cận này là sử dụng một news encoder và một user encoder. Ý tưởng như sau:</p><ul><li><p>Trong news encoder, nhóm tác giả sử dụng multi-head self-attention để học các biểu diễn tin bài từ tiêu đề bằng cách mô hình hóa sự tương tác giữa các từ.</p></li><li><p>User encoder học biểu diễn của user thông qua các tin bài đã xem của họ và sử dụng multi-head self-attention để nắm bắt sự liên quan giữa các tin bài.</p></li></ul><p>Bên cạnh đó, nhóm tác giả cũng sử dụng attention để học thêm nhiều thông tin hơn từ tin bài và biểu diễn của user bằng cách chọn các từ và tin bài quan trọng.</p><h1 id=\"3phngphp\">3. Phương pháp</h1><p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1673516496080/71594291-d1c2-4b67-84cf-a90cb9b2aaf4.png\" alt=\"\" /></p><h2 id=\"31newsencoder\">3.1. News Encoder</h2><p>Module news encoder được dùng để học biểu diễn của tin bài. Module này gồm 3 layer. Layer đầu tiên là word embedding được sử dụng để convert tiêu đề của tin bài từ một chuỗi các từ thành một chuỗi embedding vector thấp chiều (low-dimensional).</p><p>Layer thứ 2 là word-level multi-head self-attention network. Ta giả định rằng, sự tương tác giữa các từ trong tiêu đề là quan trọng cho việc học biểu diễn tin bài. Dựa vào quan sát rằng, các từ trong tiêu đề có mức độ quan trọng đối với nội dung tin bài khác nhau và một từ có thể tương tác với nhiều từ khác. Nhóm tác giả đề xuất sử dụng self-attention để học biểu diễn context của từ bằng cách capture sự tương tác giữa các từ này. Biểu diễn của từ $i<em>{th}$ được học bởi attention head $k</em>{th}$ được tính như sau:</p><p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1673516678178/68fbf1f4-5b50-438b-b23a-83da03d1bb0c.png\" alt=\"\" /></p><p>trong đó $\\mathbf{Q}^w<em>k$ và $\\mathbf{V}^w</em>k$ là projection parameters trong self-attention head $k<em>th$. $\\alpha^k</em>{i,j}$ biểu thị mức độ quan trọng tương đối của tương tác giữa từ thứ $i$ và từ thứ $j$. Biểu diễn multi-head $h^w_i$ của từ thứ $i$ là concat của các biểu diễn self-attention head riêng biệt.</p><p>Lớp thứ 3 là một neural network additive attention để chọn các từ quan trọng trong news title để học các biểu diễn thông tin tin bài hiệu quả hơn. Trọng số attention $\\alpha^w_i$ của từ thứ $i$ trong một new title được tính như sau:</p><p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1673516696574/92abb733-5022-4d0f-afe5-93059b4ce476.png\" alt=\"\" /></p><p>Trong đó $\\mathbf{V}<em>w$ và $\\mathbf{v}</em>w$ là các projection parameter và $\\mathbf{q}_w$ là query vector. Biểu diễn cuối cùng của tiêu đề tin bài là tổng có trọng số của biểu diễn context word.</p><p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1673516709576/eff9b304-e21a-4b55-b5f4-c78938507333.png\" alt=\"\" /></p><h2 id=\"32userencoder\">3.2. User Encoder</h2><p>User encoder được dùng để học biểu diễn của user từ các tin bài mà họ đã xem. Dựa trên ý tưởng các tin bài người dùng xem có sự liên quan tới nhau. Nhóm tác giả đề xuất sử dụng multi-head self-attention để nâng cao khả năng biểu diễn tin bài bằng cách capture sự tương tác giữa chúng. Biểu diễn của tin bài $i<em>{th}$ được học bởi attention head $k</em>{th}$ được tính như sau:</p><p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1673516719516/7202a8c2-fb8b-43c0-891e-62d1dff0d487.png\" alt=\"\" /></p><p>trong đó $\\mathbf{Q}^n<em>k$ và $\\mathbf{V}^n</em>k$ là các tham số của các self-attention head tin bài. $\\beta^k<em>{i,j}$ biểu diễn tương tác giữa tin bài $j</em>{th}$ và $k_{th}$.</p><p>Lớp tiếp theo là một mạng additive attention chọn tin bài quan trọng để học nhiều thông tin biểu diễn user hơn. Trọng số attention của tin bài $i_{th}$ được tính như sau:</p><p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1673516731349/1cf626f5-5d93-4149-97e2-248f34e09b1e.png\" alt=\"\" /></p><p>trong đó $\\mathbf{V}<em>n, \\mathbf{v}</em>n$ và $\\mathbf{q}_n$ là các tham số của mạng attention và $N$ là số tin bài đã xem. Biểu diễn cuối cùng của user được tính như sau:</p><p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1673516738021/8bf5c44c-f131-470a-9d0c-998d8a0dc516.png\" alt=\"\" /></p><h2 id=\"33clickpredictor\">3.3. Click Predictor</h2><p>Click predictor module được sử dụng để dự đoán xác suất người dùng click vào một tin bài ứng cử viên. Xác xuất đó được tính bằng tích vô hướng của vector biểu diễn user và vector biểu diễn tin bài.</p><p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1673925775904/dad250ea-a2b3-42b6-b5bc-42c4b80f097f.png\" alt=\"\" /></p><p>Nhóm tác giả cũng nghiên cứu các phương pháp scoring khác nhưng tích vô hướng vẫn hiệu quả và cho hiệu suất tốt nhất.</p><h2 id=\"34modeltraining\">3.4. Model Training</h2><p>Nhóm tác giải sử dụng kĩ thuật negative sampling để huấn liên model. Ý tưởng như sau:</p><ul><li><p>Với mỗi tin bài được xem bởi user (positive sample), ta lấy ngẫu nhiên $K$ mẫu tin bài cũng được hiển thị cùng lúc nhưng user không click vào (negative sample).</p></li><li><p>Shuffle thứ tự tin bài để tránh positional biases (hiểu đơn giản là vị trí tin bài trên bảng tin ảnh hưởng rất nhiều đến việc user click vào xem). Công thức tính xác suất user click vào 1 postive sample được chuẩn hóa như sau:</p></li></ul><p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1673516746020/417ac806-995c-4dd7-b992-aa14eb77c4a0.png\" alt=\"\" /></p><p>Loss function sử dụng là negative log-likelihood của tất cả positive sample $\\mathcal{S}$ được tính như sau:</p><p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1673516751631/eb1c5e0e-202b-414c-9106-95a02f226c2d.png\" alt=\"\" /></p><h1 id=\"4thcnghim\">4. Thực nghiệm</h1><p>Nhóm tác giả thực nghiệm trên bộ dữ liệu thực tế với thống kê như sau</p><p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1673516776418/d9ad707d-8174-45b3-9654-205a180fca6b.png\" alt=\"\" /></p><p>Kết quả thực nghiệm như sau, ta thấy kết quả của phương pháp sử dụng neural network tốt hơn các phương pháp truyền thống. Đặc biệt, phương pháp đề xuất NRMS tốt hơn các phương pháp còn lại.</p><p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1673516791133/8a9de494-c192-4e78-8f9d-99296ff5bb6d.png\" alt=\"\" /></p><p>Nhóm tác giả cũng thực hiện đo lường thời gian encoding với các method khác.</p><p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1673516814053/c6fa0763-36de-462a-840c-68ac72773b44.png\" alt=\"\" /></p><p>Nhóm tác giả thực hiện đó mức độ hiệu quả của việc áp dụng cơ chế attention vào method.</p><p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1673927524981/dbb7b5ae-7b60-4efe-abbf-15afc150b688.png\" alt=\"\" /></p><h1 id=\"5ktlun\">5. Kết luận</h1><p>Ý tưởng của bài báo khá clear và đơn giản. Kiến trúc sử dụng chủ yếu cơ chế attention (cụ thể là self attention và additive attention) với các quan sát rất tự nhiên như: Các tin bài của cùng 1 người dùng có sự liên quan tới nhau, Các từ trong tiêu đề của tin bài có mức độ quan trọng và tương tác khác nhau,…</p><h1 id=\"6thamkho\">6. Tham khảo</h1><p>[1] <a href=\"https://wuch15.github.io/paper/EMNLP2019-NRMS.pdf\">EMNLP2019-NRMS.pdf (</a><a href=\"http://wuch15.github.io\">wuch15.github.io</a><a href=\"https://wuch15.github.io/paper/EMNLP2019-NRMS.pdf\">)</a></p><p>[2] <a href=\"https://neptune.ai/blog/recommender-systems-metrics\">Recommender Systems: Machine Learning Metrics and Business Metrics -</a> <a href=\"http://neptune.ai\">neptune.ai</a></p><p>[3] <a href=\"https://github.com/wuch15/EMNLP2019-NRMS\">wuch15/EMNLP2019-NRMS: The source codes for the paper \"Neural News Recommendation with Multi-Head Self-Attention\". (</a><a href=\"http://github.com\">github.com</a><a href=\"https://github.com/wuch15/EMNLP2019-NRMS\">)</a></p>","title":"Paper reading | Neural News Recommendation with Multi-Head Self-Attention","tags":["Deep Learning","Recommendation System"],"created_at":1691744400000,"updated_at":1691983266000,"comments":[]}